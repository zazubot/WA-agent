{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# How does the router node work?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook shows how the router node works. This node will take care of deciding the type of response to give to the user: text, image or audio. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "ROUTER_PROMPT = \"\"\"\n",
        "You are a conversational assistant that needs to decide the type of response to give to the user.\n",
        "You'll take into account the conversation so far and determine if the best next response is \n",
        "a text message, an image or an audio message.\n",
        "\n",
        "GENERAL RULES:\n",
        "1. Always analyse the full conversation before making a decision.\n",
        "2. Only return one of the following outputs: 'conversation', 'image' or 'audio'\n",
        "\n",
        "IMPORTANT RULES FOR IMAGE GENERATION:\n",
        "1. ONLY generate an image when there is an EXPLICIT request from the user for visual content\n",
        "2. DO NOT generate images for general statements or descriptions\n",
        "3. DO NOT generate images just because the conversation mentions visual things or places\n",
        "4. The request for an image should be the main intent of the user's last message\n",
        "\n",
        "IMPORTANT RULES FOR AUDIO GENERATION:\n",
        "1. ONLY generate audio when there is an EXPLICIT request to hear Zazu's voice\n",
        "\n",
        "Output MUST be one of:\n",
        "1. 'conversation' - for normal text message responses\n",
        "2. 'image' - ONLY when user explicitly requests visual content\n",
        "3. 'audio' - ONLY when user explicitly requests voice/audio\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first thing we need is to define the chain that will be used to make the decision. Notice we are using a structured output, so that the response is always a Pydantic model containing the response type - one of 'conversation', 'image' or 'audio'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "class RouterResponse(BaseModel):\n",
        "    response_type: str = Field(description=\"The response type to give to the user. It must be one of: 'conversation', 'image' or 'audio'\")\n",
        "\n",
        "\n",
        "def get_chat_model():\n",
        "    return ChatGroq(\n",
        "        api_key=os.getenv(\"GROQ_API_KEY\"),\n",
        "        model_name=\"llama-3.3-70b-versatile\",\n",
        "        temperature=0.3,\n",
        "    ).with_structured_output(RouterResponse)\n",
        "    \n",
        "    \n",
        "def get_router_chain():\n",
        "    model = get_chat_model()\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [(\"system\", ROUTER_PROMPT), MessagesPlaceholder(variable_name=\"messages\")]\n",
        "    )\n",
        "\n",
        "    return prompt | model\n",
        "\n",
        "\n",
        "chain = get_router_chain()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have the chain, we can test it with some conversation examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 1: A normal exchange with text messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RouterResponse(response_type='conversation')"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "messages = [\n",
        "    HumanMessage(content=\"Hello, how are you?\"),\n",
        "    AIMessage(content=\"I'm fine, thank you!\"),\n",
        "    HumanMessage(content=\"My name is Ahmed by the way! You?\"),\n",
        "]\n",
        "chain.invoke({\"messages\": messages})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected, the response is of 'conversation' type - the user is not asking for any visual or audio content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 2: A request for an image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RouterResponse(response_type='image')"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "messages = [\n",
        "    HumanMessage(content=\"Hello, I'm Ahmed!\"),\n",
        "    AIMessage(content=\"Nice to meet you Ahmed! I'm Zazu, nice to meet you too!\"),\n",
        "    HumanMessage(content=\"So what are you doing right now?\"),\n",
        "    AIMessage(content=\"I'm looking at a very beautiful landscape from my window. It's a very nice day today!\"),\n",
        "    HumanMessage(content=\"Send me a picture of that!\")\n",
        "]\n",
        "chain.invoke({\"messages\": messages})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 3: A request for an audio message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RouterResponse(response_type='audio')"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "messages = [\n",
        "    HumanMessage(content=\"Do you have any hobbies?\"),\n",
        "    AIMessage(content=\"I like to sing when I'm alone. You?\"),\n",
        "    HumanMessage(content=\"Really? Now that you mention it, I've never heard your voice!\"),\n",
        "]\n",
        "chain.invoke({\"messages\": messages})"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
